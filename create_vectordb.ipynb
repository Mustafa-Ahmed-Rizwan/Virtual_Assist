{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run this code in google colab:\n",
        "- first select t4 gpu in runtime then go forward and run these codes \n",
        "- then download the vector_db.zip file and paste the vector db folder in your project directory\n",
        "- upload arena_data_en.jsonl and also arena_data_de.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EfiFLDm6jt8",
        "outputId": "51c462e4-c8aa-406b-85cf-10ffa635e8d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.3.72)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.34.1)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.1)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.11.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=f0abe53ab2cf31043852deb9d4845cf749e8d98632146ef51251b01a233bc978\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, python-dotenv, pybase64, overrides, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, humanfriendly, httpx-sse, httptools, bcrypt, backoff, watchfiles, typing-inspect, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, pydantic-settings, opentelemetry-semantic-conventions, onnxruntime, kubernetes, dataclasses-json, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, langchain-huggingface, chromadb, langchain-community\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.15 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 httptools-0.6.4 httpx-sse-0.4.1 humanfriendly-10.0 kubernetes-33.1.0 langchain-community-0.3.27 langchain-huggingface-0.3.1 marshmallow-3.26.1 mmh3-5.2.0 mypy-extensions-1.1.0 onnxruntime-1.22.1 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.2 pydantic-settings-2.10.1 pypika-0.48.9 python-dotenv-1.1.1 typing-inspect-0.9.0 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-huggingface langchain-community chromadb tqdm \"pinecone[grpc]\" langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxKqQ0Ei6kq8",
        "outputId": "ba6af831-97c9-44a5-b74d-4573e9ac78b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain_experimental) (0.3.27)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.11/dist-packages (from langchain_experimental) (0.3.72)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.8)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.11.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.9)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.1)\n",
            "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_experimental\n",
            "Successfully installed langchain_experimental-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcnOGIWj6koy",
        "outputId": "69fb650c-9ac8-4b0f-c443-6228a6210f22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: sentence-transformers 4.1.0\n",
            "Uninstalling sentence-transformers-4.1.0:\n",
            "  Successfully uninstalled sentence-transformers-4.1.0\n",
            "Found existing installation: transformers 4.54.0\n",
            "Uninstalling transformers-4.54.0:\n",
            "  Successfully uninstalled transformers-4.54.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y torch torchvision sentence-transformers transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXSdeOUu6kmf",
        "outputId": "3c04c3ea-0b68-4146-8920-91df60aa0b0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (905.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m905.3/905.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.16.0 requires transformers, which is not installed.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1+cu118 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.3 torch-2.7.1+cu118 torchvision-0.22.1+cu118 triton-3.3.1\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.7.1+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.2/470.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.54.1-py3-none-any.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers, sentence-transformers\n",
            "Successfully installed sentence-transformers-5.0.0 transformers-4.54.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install sentence-transformers transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# before going forward make sure you restart the session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# get api key from pinecone\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"\"\n",
        "os.environ[\"PINECONE_REGION\"] = \"us-east-1\"   # or your region\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "jVqydOeE6kkq",
        "outputId": "cd1a12a7-8db5-4637-d72c-4110ceb9f6a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading documents: 100%|██████████| 25.0M/25.0M [00:00<00:00, 159MB/s]\n",
            "Semantic chunking documents: 100%|██████████| 296/296 [05:58<00:00,  1.21s/it]\n",
            "/tmp/ipython-input-4015048554.py:101: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectorstore = Chroma(\n",
            "Indexing batches: 100%|██████████| 95/95 [01:47<00:00,  1.13s/it]\n",
            "/tmp/ipython-input-4015048554.py:110: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist()\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_ea0b2bb0-177b-4a43-a863-977c9c2c568f\", \"vector_db.zip\", 41017118)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import hashlib\n",
        "import json\n",
        "import shutil\n",
        "import logging\n",
        "import time\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Set\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from langchain.schema import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "# from langchain.vectorstores import Chroma  # COMMENTED OUT\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import files  # For download in Colab\n",
        "\n",
        "# Configuration for both collections\n",
        "CONFIG = {\n",
        "    \"embedding_models\": {\n",
        "        \"en\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "        \"de\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "    },\n",
        "    # \"persist_directory\": \"/content/vector_db\",  # COMMENTED OUT - Not needed for Pinecone\n",
        "    \"pinecone\": {\n",
        "        \"environment\": \"us-east-1\",  # Change to your preferred region\n",
        "        \"index_name_prefix\": \"arena2036\",  # Will create arena2036-en and arena2036-de\n",
        "        \"dimension\": 768,  # Dimension for all-mpnet-base-v2 model\n",
        "        \"metric\": \"cosine\"\n",
        "    },\n",
        "    \"collections\": {\n",
        "        \"en\": {\n",
        "            \"name\": \"arena2036_en\",\n",
        "            \"input_file\": \"arena_data_en.jsonl\",\n",
        "            \"index_name\": \"arena2036-en\"\n",
        "        },\n",
        "        \"de\": {\n",
        "            \"name\": \"arena2036_de\", \n",
        "            \"input_file\": \"arena_data_de.jsonl\",\n",
        "            \"index_name\": \"arena2036-de\"\n",
        "        }\n",
        "    },\n",
        "    \"chunking\": {\n",
        "        \"breakpoint_threshold_type\": \"percentile\",\n",
        "        \"breakpoint_threshold_amount\": 80\n",
        "    },\n",
        "    # Reduced batch size to prevent memory issues\n",
        "    \"batch_size\": 16,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"normalize_embeddings\": True\n",
        "}\n",
        "\n",
        "# Logging setup with Colab compatibility\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    handlers=[logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Helper function to ensure logs show up in Colab\n",
        "def log_and_flush(message, level=\"info\"):\n",
        "    if level == \"info\":\n",
        "        logger.info(message)\n",
        "    elif level == \"warning\":\n",
        "        logger.warning(message)\n",
        "    elif level == \"error\":\n",
        "        logger.error(message)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def setup_pinecone():\n",
        "    \"\"\"Initialize Pinecone client and create indexes if they don't exist\"\"\"\n",
        "    # Get API key from environment variable\n",
        "    api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"PINECONE_API_KEY environment variable not set!\")\n",
        "    \n",
        "    # Initialize Pinecone\n",
        "    pc = Pinecone(api_key=api_key)\n",
        "    \n",
        "    # Create indexes for both languages if they don't exist\n",
        "    for language in [\"en\", \"de\"]:\n",
        "        index_name = CONFIG[\"collections\"][language][\"index_name\"]\n",
        "        \n",
        "        try:\n",
        "            # Check if index exists\n",
        "            existing_indexes = [idx.name for idx in pc.list_indexes()]\n",
        "            \n",
        "            if index_name not in existing_indexes:\n",
        "                log_and_flush(f\"Creating Pinecone index: {index_name}\")\n",
        "                pc.create_index(\n",
        "                    name=index_name,\n",
        "                    dimension=CONFIG[\"pinecone\"][\"dimension\"],\n",
        "                    metric=CONFIG[\"pinecone\"][\"metric\"],\n",
        "                    spec=ServerlessSpec(\n",
        "                        cloud=\"aws\",\n",
        "                        region=CONFIG[\"pinecone\"][\"environment\"]\n",
        "                    )\n",
        "                )\n",
        "                # Wait for index to be ready\n",
        "                while not pc.describe_index(index_name).status['ready']:\n",
        "                    log_and_flush(f\"Waiting for index {index_name} to be ready...\")\n",
        "                    time.sleep(1)\n",
        "                log_and_flush(f\"Index {index_name} created and ready!\")\n",
        "            else:\n",
        "                log_and_flush(f\"Index {index_name} already exists\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            log_and_flush(f\"Error with index {index_name}: {e}\", \"error\")\n",
        "            raise\n",
        "    \n",
        "    return pc\n",
        "\n",
        "def make_source_id(url: str, title: str) -> str:\n",
        "    \"\"\"Create a unique source ID from URL and title\"\"\"\n",
        "    base = (url or \"\") + \"|\" + (title or \"\")\n",
        "    h = hashlib.sha1(base.encode(\"utf-8\")).hexdigest()\n",
        "    return h[:14]\n",
        "\n",
        "def flatten_documents(maybe_nested):\n",
        "    \"\"\"Safe flatten utility (in case splitter returns nested lists)\"\"\"\n",
        "    out = []\n",
        "    for item in maybe_nested:\n",
        "        if item is None:\n",
        "            continue\n",
        "        if isinstance(item, list):\n",
        "            out.extend(flatten_documents(item))\n",
        "        else:\n",
        "            out.append(item)\n",
        "    return out\n",
        "\n",
        "def load_documents(file_path: str, language: str) -> Tuple[List[Document], int, List[str]]:\n",
        "    \"\"\"Load documents with comprehensive tracking and lenient parsing\"\"\"\n",
        "    documents = []\n",
        "    failed_lines = []\n",
        "    total_lines = 0\n",
        "\n",
        "    # First pass: count total lines for accurate tracking\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            total_lines = sum(1 for line in f if line.strip())\n",
        "        log_and_flush(f\"Total lines in file: {total_lines}\")\n",
        "    except Exception as e:\n",
        "        log_and_flush(f\"Failed to count lines in file: {e}\", \"error\")\n",
        "        raise\n",
        "\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            line_num = 0\n",
        "            for line in tqdm(f, desc=f\"Loading {language} documents\", total=total_lines):\n",
        "                line_num += 1\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    rec = json.loads(line)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    error_msg = f\"Line {line_num}: Invalid JSON - {e}\"\n",
        "                    log_and_flush(error_msg, \"error\")\n",
        "                    failed_lines.append(error_msg)\n",
        "                    continue\n",
        "\n",
        "                # More lenient content extraction\n",
        "                content = (rec.get(\"content\") or \"\").strip()\n",
        "                title = (rec.get(\"title\") or \"\").strip()\n",
        "                url = (rec.get(\"url\") or \"\").strip()\n",
        "\n",
        "                # Only skip if ALL fields are empty (much more lenient)\n",
        "                if not content and not title and not url:\n",
        "                    error_msg = f\"Line {line_num}: All fields empty (url, title, content)\"\n",
        "                    log_and_flush(error_msg, \"warning\")\n",
        "                    failed_lines.append(error_msg)\n",
        "                    continue\n",
        "\n",
        "                # Create document even with minimal content\n",
        "                # If content is empty but title exists, use title as content\n",
        "                if not content and title:\n",
        "                    combined = title\n",
        "                    log_and_flush(f\"Using title as content for URL: {url}\")\n",
        "                elif content and title:\n",
        "                    combined = f\"{title}\\n{content}\"\n",
        "                elif content:\n",
        "                    combined = content\n",
        "                else:\n",
        "                    # Last resort: use URL as content if nothing else\n",
        "                    combined = url or f\"Document from line {line_num}\"\n",
        "                    log_and_flush(f\"Using URL as content for line {line_num}\")\n",
        "\n",
        "                source_id = make_source_id(url, title)\n",
        "                metadata = {\n",
        "                    \"url\": url,\n",
        "                    \"title\": title,\n",
        "                    \"source\": Path(url).stem if url else f\"line_{line_num}\",\n",
        "                    \"language\": language,\n",
        "                    \"source_id\": source_id,\n",
        "                    \"original_line\": line_num\n",
        "                }\n",
        "\n",
        "                documents.append(Document(page_content=combined, metadata=metadata))\n",
        "\n",
        "    except Exception as e:\n",
        "        log_and_flush(f\"Failed to load documents: {e}\", \"error\")\n",
        "        raise\n",
        "\n",
        "    log_and_flush(f\"Successfully loaded {len(documents)} documents out of {total_lines} total lines\")\n",
        "    if failed_lines:\n",
        "        log_and_flush(f\"Failed to process {len(failed_lines)} lines:\", \"warning\")\n",
        "        for failed in failed_lines[:10]:  # Show first 10 failures\n",
        "            log_and_flush(f\"  {failed}\", \"warning\")\n",
        "        if len(failed_lines) > 10:\n",
        "            log_and_flush(f\"  ... and {len(failed_lines) - 10} more failures\", \"warning\")\n",
        "\n",
        "    return documents, len(documents), failed_lines\n",
        "\n",
        "def safe_chunk_documents(text_splitter, docs: List[Document]) -> List[Document]:\n",
        "    \"\"\"Chunk documents with individual fallback for problematic docs\"\"\"\n",
        "    all_chunks = []\n",
        "    failed_docs = []\n",
        "\n",
        "    log_and_flush(\"Starting document chunking process...\")\n",
        "\n",
        "    # Try batch chunking first\n",
        "    try:\n",
        "        log_and_flush(\"Attempting batch chunking...\")\n",
        "        raw_chunks = text_splitter.split_documents(docs)\n",
        "        chunks = flatten_documents(raw_chunks)\n",
        "        log_and_flush(f\"Batch chunking successful: {len(chunks)} chunks created\")\n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        log_and_flush(f\"Batch chunking failed: {e}. Falling back to individual document processing...\", \"warning\")\n",
        "\n",
        "    # Individual document processing fallback\n",
        "    for i, doc in enumerate(tqdm(docs, desc=\"Chunking documents individually\")):\n",
        "        try:\n",
        "            doc_chunks = text_splitter.split_documents([doc])\n",
        "            doc_chunks = flatten_documents(doc_chunks)\n",
        "\n",
        "            # If no chunks created, create one from original document\n",
        "            if not doc_chunks:\n",
        "                log_and_flush(f\"No chunks created for doc {i}, using original document\", \"warning\")\n",
        "                # Create a chunk from the original document\n",
        "                chunk = Document(\n",
        "                    page_content=doc.page_content,\n",
        "                    metadata={**doc.metadata, \"chunk_method\": \"fallback_original\"}\n",
        "                )\n",
        "                doc_chunks = [chunk]\n",
        "\n",
        "            all_chunks.extend(doc_chunks)\n",
        "\n",
        "        except Exception as e:\n",
        "            log_and_flush(f\"Failed to chunk document {i} (source_id: {doc.metadata.get('source_id', 'unknown')}): {e}\", \"error\")\n",
        "            # Create a fallback chunk from the original document\n",
        "            try:\n",
        "                fallback_chunk = Document(\n",
        "                    page_content=doc.page_content[:8000],  # Truncate if too long\n",
        "                    metadata={**doc.metadata, \"chunk_method\": \"fallback_truncated\"}\n",
        "                )\n",
        "                all_chunks.append(fallback_chunk)\n",
        "                log_and_flush(f\"Created fallback chunk for document {i}\")\n",
        "            except Exception as e2:\n",
        "                log_and_flush(f\"Even fallback chunk creation failed for document {i}: {e2}\", \"error\")\n",
        "                failed_docs.append(doc.metadata.get('source_id', f'doc_{i}'))\n",
        "\n",
        "    log_and_flush(f\"Individual chunking complete: {len(all_chunks)} chunks created, {len(failed_docs)} documents failed completely\")\n",
        "    if failed_docs:\n",
        "        log_and_flush(f\"Completely failed document IDs: {failed_docs}\", \"warning\")\n",
        "\n",
        "    return all_chunks\n",
        "\n",
        "def create_collection(language: str, config: dict, pc: Pinecone):\n",
        "    \"\"\"Create a new collection for a specific language with comprehensive tracking\"\"\"\n",
        "    \n",
        "    # Load documents\n",
        "    input_file = config[\"collections\"][language][\"input_file\"]\n",
        "    docs, doc_count, failed_lines = load_documents(input_file, language)\n",
        "    \n",
        "    if not docs:\n",
        "        raise ValueError(f\"No {language} documents loaded!\")\n",
        "    \n",
        "    log_and_flush(f\"Loaded {doc_count} documents for language={language}\")\n",
        "\n",
        "    # Track original source IDs for verification\n",
        "    original_source_ids = {doc.metadata[\"source_id\"] for doc in docs}\n",
        "    log_and_flush(f\"Tracking {len(original_source_ids)} unique source IDs\")\n",
        "\n",
        "    # Create sources index\n",
        "    sources_index = {\n",
        "        d.metadata[\"source_id\"]: {\n",
        "            \"title\": d.metadata.get(\"title\", \"\"),\n",
        "            \"url\": d.metadata.get(\"url\", \"\"),\n",
        "            \"language\": d.metadata.get(\"language\", \"\"),\n",
        "            \"original_line\": d.metadata.get(\"original_line\", \"\")\n",
        "        } for d in docs\n",
        "    }\n",
        "\n",
        "    # Initialize embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=config[\"embedding_models\"][language],\n",
        "        model_kwargs={\"device\": config[\"device\"]},\n",
        "        encode_kwargs={\"normalize_embeddings\": config[\"normalize_embeddings\"]}\n",
        "    )\n",
        "\n",
        "    # Create semantic text splitter\n",
        "    text_splitter = SemanticChunker(\n",
        "        embeddings,\n",
        "        breakpoint_threshold_type=config[\"chunking\"][\"breakpoint_threshold_type\"],\n",
        "        breakpoint_threshold_amount=config[\"chunking\"][\"breakpoint_threshold_amount\"]\n",
        "    )\n",
        "\n",
        "    log_and_flush(\"Starting robust document chunking...\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    chunks = safe_chunk_documents(text_splitter, docs)\n",
        "\n",
        "    # Clean chunks more carefully\n",
        "    clean_chunks = []\n",
        "    for i, c in enumerate(chunks):\n",
        "        content = (c.page_content or \"\").strip()\n",
        "        if not content:\n",
        "            log_and_flush(f\"Empty chunk {i} found, skipping\", \"warning\")\n",
        "            continue\n",
        "\n",
        "        # Ensure metadata is complete\n",
        "        metadata = c.metadata or {}\n",
        "        if \"source_id\" not in metadata or not metadata[\"source_id\"]:\n",
        "            # Try to reconstruct source_id from other metadata\n",
        "            url = metadata.get(\"url\", \"\")\n",
        "            title = metadata.get(\"title\", \"\")\n",
        "            metadata[\"source_id\"] = make_source_id(url, title)\n",
        "            log_and_flush(f\"Reconstructed source_id for chunk {i}: {metadata['source_id']}\")\n",
        "\n",
        "        c.metadata = metadata\n",
        "        clean_chunks.append(c)\n",
        "\n",
        "    chunks = clean_chunks\n",
        "    total_chunks = len(chunks)\n",
        "    log_and_flush(f\"Final chunk count: {total_chunks} (after cleaning)\")\n",
        "\n",
        "    # Verify we have chunks for all original documents\n",
        "    chunk_source_ids = {chunk.metadata.get(\"source_id\") for chunk in chunks}\n",
        "    missing_source_ids = original_source_ids - chunk_source_ids\n",
        "    if missing_source_ids:\n",
        "        log_and_flush(f\"Missing chunks for {len(missing_source_ids)} source documents!\", \"warning\")\n",
        "        log_and_flush(f\"Missing source IDs (first 10): {list(missing_source_ids)[:10]}\", \"warning\")\n",
        "\n",
        "        # Try to create fallback chunks for missing documents\n",
        "        for doc in docs:\n",
        "            if doc.metadata[\"source_id\"] in missing_source_ids:\n",
        "                log_and_flush(f\"Creating fallback chunk for missing source: {doc.metadata['source_id']}\")\n",
        "                fallback_chunk = Document(\n",
        "                    page_content=doc.page_content[:8000],  # Truncate if needed\n",
        "                    metadata={**doc.metadata, \"chunk_method\": \"missing_fallback\"}\n",
        "                )\n",
        "                chunks.append(fallback_chunk)\n",
        "\n",
        "        total_chunks = len(chunks)\n",
        "        log_and_flush(f\"After adding fallback chunks: {total_chunks} total chunks\")\n",
        "\n",
        "    # Sample chunks for verification\n",
        "    for s in range(min(3, total_chunks)):\n",
        "        snippet = chunks[s].page_content[:200].replace(\"\\n\", \" \")\n",
        "        log_and_flush(f\"Sample chunk {s}: {snippet}... | source_id: {chunks[s].metadata.get('source_id', 'N/A')}\")\n",
        "\n",
        "    # PINECONE IMPLEMENTATION STARTS HERE\n",
        "    index_name = config[\"collections\"][language][\"index_name\"]\n",
        "    \n",
        "    # Get the Pinecone index\n",
        "    try:\n",
        "        index = pc.Index(index_name)\n",
        "        log_and_flush(f\"Connected to Pinecone index: {index_name}\")\n",
        "        \n",
        "        # Check index stats\n",
        "        stats = index.describe_index_stats()\n",
        "        log_and_flush(f\"Index {index_name} current vector count: {stats['total_vector_count']}\")\n",
        "        \n",
        "        # Optional: Clear existing vectors if you want a fresh start\n",
        "        # Uncomment the next line if you want to clear the index\n",
        "        # index.delete(delete_all=True)\n",
        "        \n",
        "    except Exception as e:\n",
        "        log_and_flush(f\"Error connecting to Pinecone index {index_name}: {e}\", \"error\")\n",
        "        raise\n",
        "\n",
        "    # Create PineconeVectorStore\n",
        "    vectorstore = PineconeVectorStore(\n",
        "        index=index,\n",
        "        embedding=embeddings,\n",
        "        text_key=\"text\",  # Field name for storing text content\n",
        "        namespace=language  # Use language as namespace for organization\n",
        "    )\n",
        "\n",
        "    # Ultra-robust document addition with retry logic\n",
        "    batch_size = max(1, int(config.get(\"batch_size\", 16)))\n",
        "    added_count = 0\n",
        "    failed_chunks = []\n",
        "    retry_chunks = []\n",
        "    global_idx = 0\n",
        "    chunks_index = {}\n",
        "    successfully_added_source_ids: Set[str] = set()\n",
        "\n",
        "    log_and_flush(f\"Adding {total_chunks} chunks to Pinecone with batch size {batch_size}...\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    # Primary batch processing\n",
        "    for start in tqdm(range(0, total_chunks, batch_size), desc=\"Adding batches\"):\n",
        "        batch = chunks[start : start + batch_size]\n",
        "        batch_ids = []\n",
        "        batch_source_ids = []\n",
        "        \n",
        "        for c in batch:\n",
        "            src_id = c.metadata.get(\"source_id\") or make_source_id(\n",
        "                c.metadata.get(\"url\", \"\"), \n",
        "                c.metadata.get(\"title\", \"\")\n",
        "            )\n",
        "            chunk_id = f\"{src_id}_chunk_{global_idx}\"\n",
        "            global_idx += 1\n",
        "            c.metadata[\"chunk_id\"] = chunk_id\n",
        "            batch_ids.append(chunk_id)\n",
        "            batch_source_ids.append(src_id)\n",
        "            \n",
        "            chunks_index[chunk_id] = {\n",
        "                \"source_id\": src_id,\n",
        "                \"title\": c.metadata.get(\"title\", \"\"),\n",
        "                \"url\": c.metadata.get(\"url\", \"\"),\n",
        "                \"snippet\": c.page_content[:300]\n",
        "            }\n",
        "\n",
        "        # Try batch addition\n",
        "        success = False\n",
        "        try:\n",
        "            vectorstore.add_documents(batch, ids=batch_ids)\n",
        "            added_count += len(batch)\n",
        "            successfully_added_source_ids.update(batch_source_ids)\n",
        "            success = True\n",
        "            logger.debug(f\"Batch {start//batch_size + 1} added successfully ({len(batch)} docs)\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Batch {start//batch_size + 1} failed: {e}. Will retry individually.\")\n",
        "\n",
        "        # If batch failed, add to retry queue\n",
        "        if not success:\n",
        "            for i, doc in enumerate(batch):\n",
        "                retry_chunks.append((doc, batch_ids[i], batch_source_ids[i]))\n",
        "\n",
        "    # Retry failed chunks individually with exponential backoff\n",
        "    log_and_flush(f\"Retrying {len(retry_chunks)} failed chunks individually...\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    for doc, doc_id, src_id in tqdm(retry_chunks, desc=\"Retrying individual chunks\"):\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                vectorstore.add_documents([doc], ids=[doc_id])\n",
        "                added_count += 1\n",
        "                successfully_added_source_ids.add(src_id)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if attempt == max_retries - 1:\n",
        "                    logger.error(f\"Failed to add chunk {doc_id} after {max_retries} attempts: {e}\")\n",
        "                    failed_chunks.append(doc_id)\n",
        "                else:\n",
        "                    logger.debug(f\"Retry {attempt + 1} failed for chunk {doc_id}: {e}\")\n",
        "                    time.sleep(0.1 * (2 ** attempt))  # Exponential backoff\n",
        "\n",
        "    # Final verification of source coverage\n",
        "    missing_from_vector = original_source_ids - successfully_added_source_ids\n",
        "    if missing_from_vector:\n",
        "        log_and_flush(f\"CRITICAL: {len(missing_from_vector)} source documents missing from vector DB!\", \"error\")\n",
        "        log_and_flush(f\"Missing source IDs: {list(missing_from_vector)[:20]}\", \"error\")\n",
        "    else:\n",
        "        log_and_flush(\"✅ SUCCESS: All original source documents are represented in the vector database!\")\n",
        "    \n",
        "    sys.stdout.flush()\n",
        "\n",
        "    # Save comprehensive indexes to local files for backup\n",
        "    try:\n",
        "        # Create local backup directory\n",
        "        backup_dir = Path(\"/content/pinecone_backup\")\n",
        "        backup_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        collection_name = config[\"collections\"][language][\"name\"]\n",
        "        \n",
        "        # Sources index\n",
        "        src_path = backup_dir / f\"{collection_name}_sources_index.json\"\n",
        "        with open(src_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(sources_index, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Chunks index\n",
        "        chunks_path = backup_dir / f\"{collection_name}_chunks_index.json\"\n",
        "        with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(chunks_index, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Processing report\n",
        "        report = {\n",
        "            \"total_input_lines\": len(docs) + len(failed_lines),\n",
        "            \"failed_to_parse\": len(failed_lines),\n",
        "            \"documents_loaded\": doc_count,\n",
        "            \"original_source_ids\": len(original_source_ids),\n",
        "            \"total_chunks_created\": total_chunks,\n",
        "            \"chunks_added_to_vector_db\": added_count,\n",
        "            \"failed_chunks\": len(failed_chunks),\n",
        "            \"successfully_added_source_ids\": len(successfully_added_source_ids),\n",
        "            \"missing_from_vector_db\": len(missing_from_vector),\n",
        "            \"missing_source_ids\": list(missing_from_vector),\n",
        "            \"pinecone_index\": index_name,\n",
        "            \"namespace\": language\n",
        "        }\n",
        "\n",
        "        report_path = backup_dir / f\"{collection_name}_processing_report.json\"\n",
        "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(report, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        log_and_flush(f\"Saved indexes and processing report to {backup_dir}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        log_and_flush(f\"Failed to write index files: {e}\", \"error\")\n",
        "\n",
        "    # Final summary\n",
        "    print(\"=\"*80)\n",
        "    print(f\"FINAL PROCESSING SUMMARY FOR {language.upper()} COLLECTION:\")\n",
        "    print(f\"📁 Input file lines: {len(docs) + len(failed_lines)}\")\n",
        "    print(f\"📄 Documents loaded: {doc_count}\")\n",
        "    print(f\"🔗 Unique source IDs: {len(original_source_ids)}\")\n",
        "    print(f\"📝 Total chunks created: {total_chunks}\")\n",
        "    print(f\"✅ Chunks added to Pinecone: {added_count}\")\n",
        "    print(f\"❌ Failed chunks: {len(failed_chunks)}\")\n",
        "    print(f\"🎯 Source coverage: {len(successfully_added_source_ids)}/{len(original_source_ids)}\")\n",
        "    print(f\"🌲 Pinecone Index: {index_name}\")\n",
        "    print(f\"📦 Namespace: {language}\")\n",
        "    if missing_from_vector:\n",
        "        print(f\"⚠️ MISSING SOURCES: {len(missing_from_vector)} documents not in vector DB\")\n",
        "    else:\n",
        "        print(\"🎉 COMPLETE SUCCESS: All source documents represented in vector DB!\")\n",
        "    print(\"=\"*80)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    logger.info(f\"Created {language} collection with {added_count} chunks from {len(successfully_added_source_ids)} sources in Pinecone index {index_name}\")\n",
        "\n",
        "def verify_collections(config: dict, pc: Pinecone):\n",
        "    \"\"\"Verify all Pinecone indexes exist and show stats\"\"\"\n",
        "    logger.info(\"Current Pinecone indexes:\")\n",
        "    try:\n",
        "        indexes = pc.list_indexes()\n",
        "        for idx_info in indexes:\n",
        "            index_name = idx_info.name\n",
        "            index = pc.Index(index_name)\n",
        "            stats = index.describe_index_stats()\n",
        "            \n",
        "            logger.info(f\"- {index_name}: {stats['total_vector_count']} vectors\")\n",
        "            \n",
        "            # Show namespace stats if available\n",
        "            if 'namespaces' in stats:\n",
        "                for ns_name, ns_stats in stats['namespaces'].items():\n",
        "                    logger.info(f\"  └─ Namespace '{ns_name}': {ns_stats['vector_count']} vectors\")\n",
        "                    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to verify Pinecone indexes: {e}\")\n",
        "\n",
        "def download_backup_files():\n",
        "    \"\"\"Download the backup index files\"\"\"\n",
        "    try:\n",
        "        backup_dir = Path(\"/content/pinecone_backup\")\n",
        "        if backup_dir.exists():\n",
        "            # Create ZIP archive of backup files\n",
        "            zip_path = \"/content/pinecone_backup\"\n",
        "            logger.info(f\"Creating ZIP archive of backup files...\")\n",
        "            shutil.make_archive(zip_path, \"zip\", backup_dir)\n",
        "            \n",
        "            # Download the file\n",
        "            logger.info(\"Downloading backup files...\")\n",
        "            files.download(f\"{zip_path}.zip\")\n",
        "            logger.info(\"Backup files downloaded successfully!\")\n",
        "        else:\n",
        "            logger.warning(\"No backup directory found to download\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to create/download backup: {e}\")\n",
        "        raise\n",
        "\n",
        "def main():\n",
        "    logger.info(\"Starting ROBUST Pinecone vector database creation process\")\n",
        "    logger.info(\"This version includes comprehensive tracking to prevent document loss\")\n",
        "    \n",
        "    # Setup Pinecone client and create indexes\n",
        "    pc = setup_pinecone()\n",
        "    \n",
        "    # Create collections for both languages\n",
        "    for language in [\"en\", \"de\"]:\n",
        "        if os.path.exists(CONFIG[\"collections\"][language][\"input_file\"]):\n",
        "            logger.info(f\"Creating {language} collection in Pinecone...\")\n",
        "            start_time = time.time()\n",
        "            create_collection(language, CONFIG, pc)\n",
        "            elapsed = time.time() - start_time\n",
        "            logger.info(f\"{language} collection created in {elapsed:.2f} seconds\")\n",
        "        else:\n",
        "            logger.warning(f\"Input file not found for {language}: {CONFIG['collections'][language]['input_file']}\")\n",
        "\n",
        "    # Verify both collections exist\n",
        "    verify_collections(CONFIG, pc)\n",
        "    \n",
        "    # Download backup files\n",
        "    download_backup_files()\n",
        "\n",
        "    logger.info(\"🎉 Pinecone vector database creation completed successfully!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
